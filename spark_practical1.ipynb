{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#    If you choose to run this notebook on your local machine \n",
    "#   set the Pyspark Set Spark environment variables as shown below\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/homebrew/Cellar/apache-spark/3.5.4/libexec\" # Your spark hom dir\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/martin/miniforge3/bin/python\"# your Python dir\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/Users/martin/miniforge3/bin/python\"# your Python dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If You choose to open on Google Colab, comment the above cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# MAPREDUCE & PYTHON FUNCTIONAL EXAMPLES\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful modules\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from collections import defaultdict\n",
    "\n",
    "#=================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Simulate MapReduce Word Count in Pure Python \n",
    "\n",
    "lines = [\"hello world\", \"hi Spark\", \"hello MapReduce\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Map phase - emit (word, 1)\n",
    "mapped = [(word, 1) for line in lines for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Shuffle phase - group by word\n",
    "grouped = defaultdict(list) # Initialize to take a list\n",
    "for word, count in mapped:\n",
    "    grouped[word].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Reduce phase - sum values for each word\n",
    "reduced = {word: sum(counts) for word, counts in grouped.items()}\n",
    "print(\"Word Count Result:\", reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Higher-Order Functions Examples \n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "# map: square each number\n",
    "squared = list(map(lambda x: x**2, numbers))\n",
    "\n",
    "# filter: retain even numbers\n",
    "evens = list(filter(lambda x: x % 2 == 0, numbers))\n",
    "\n",
    "# reduce: sum all numbers\n",
    "summed = reduce(lambda x, y: x + y, numbers)\n",
    "\n",
    "print(\"Squared:\", squared)\n",
    "print(\"Evens:\", evens)\n",
    "print(\"Summed:\", summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More examples using map, filter, reduce\n",
    "names = [\"alice\", \"bob\", \"charlie\"]\n",
    "capitalized = list(map(str.capitalize, names))\n",
    "print(\"Capitalized Names:\", capitalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chained map + filter: square even numbers\n",
    "chained = list(map(lambda x: x**2, filter(lambda x: x % 2 == 0, numbers)))\n",
    "print(\"Chained Square of Evens:\", chained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# SPARK (PySpark) EXAMPLES [The above were manual implimentations]\n",
    "# =========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataExamples\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- RDD Examples -----\n",
    "\n",
    "# Create RDD\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# Map: square values\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Filter: keep squares greater than 20\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 20)\n",
    "\n",
    "# Reduce: sum remaining squares\n",
    "sum_of_filtered = filtered_rdd.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(\"Squared RDD:\", squared_rdd.collect())\n",
    "print(\"Filtered RDD (>20):\", filtered_rdd.collect())\n",
    "print(\"Sum of Filtered:\", sum_of_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel transformation of multiple RDDs\n",
    "names_rdd = spark.sparkContext.parallelize([\"alice\", \"bob\", \"carol\"])\n",
    "lengths_rdd = names_rdd.map(lambda name: (name, len(name)))\n",
    "print(\"Name Lengths:\", lengths_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MapReduce Examples in PySpark (Real-World Scenarios) \n",
    "\n",
    "# 1. Word count on filtered text (excluding stopwords)\n",
    "text_rdd = spark.sparkContext.parallelize([\"spark is awesome\", \"pyspark handles big data\", \"big data tools\"])\n",
    "stopwords = {\"is\", \"big\", \"data\"}\n",
    "words_filtered = text_rdd.flatMap(lambda line: line.split()) \\\n",
    "                            .filter(lambda word: word.lower() not in stopwords)\n",
    "pairs = words_filtered.map(lambda word: (word.lower(), 1))\n",
    "word_counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Filtered Word Count:\", word_counts.collect())\n",
    "\n",
    "# 2. Grouping and summing based on key prefix\n",
    "pairs = spark.sparkContext.parallelize([(\"apple\", 2), (\"banana\", 3), (\"apricot\", 5), (\"berry\", 4)])\n",
    "# Group by first letter of fruit name\n",
    "grouped = pairs.map(lambda x: (x[0][0], x[1])).reduceByKey(lambda a, b: a + b)\n",
    "print(\"Grouped by Prefix:\", grouped.collect())\n",
    "\n",
    "# 3. Calculating average length of lines\n",
    "lines = spark.sparkContext.parallelize([\"hello spark\", \"map reduce model\", \"python is cool\"])\n",
    "lengths = lines.map(lambda line: (\"avg\", (len(line), 1)))\n",
    "avg_length = lengths.reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1])) \\\n",
    "                        .mapValues(lambda x: x[0]/x[1])\n",
    "print(\"Average Line Length:\", avg_length.collect())\n",
    "\n",
    "# 4. Count words only longer than 5 characters\n",
    "long_words = text_rdd.flatMap(lambda line: line.split()) \\\n",
    "                      .filter(lambda word: len(word) > 5)\n",
    "long_word_pairs = long_words.map(lambda word: (word.lower(), 1))\n",
    "long_word_counts = long_word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Long Word Counts:\", long_word_counts.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# LOAD AND SAVE DIFFERENT FILE TYPES IN SPARK\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load CSV File ---\n",
    "csv_df = spark.read.csv(\"sample_people.csv\", header=True, inferSchema=True) # replace with your path to csv\n",
    "csv_df.show()\n",
    "csv_df.printSchema()\n",
    "# Note. You can also use 'sc = spark.sparkContext' to load the data as we did in class\n",
    "# --- Load JSON File ---\n",
    "json_df = spark.read.json(\"sample_data.json\") # Replace with your path to json file\n",
    "json_df.show()\n",
    "json_df.printSchema()\n",
    "# Note. You can also use 'sc = spark.sparkContext' to load the data as we did in class\n",
    "# --- Save to Parquet Format ---\n",
    "csv_df.write.mode(\"overwrite\").parquet(\"output/people_parquet\")\n",
    "\n",
    "# --- Save to JSON Format ---\n",
    "csv_df.write.mode(\"overwrite\").json(\"output/people_json\")\n",
    "\n",
    "# --- Save to CSV Format with header ---\n",
    "csv_df.write.mode(\"overwrite\").csv(\"output/people_csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# LOG ANALYSIS USING MAPREDUCE IN PYSPARK (Step by step example to process unstructured data\n",
    "# Similar to what we did in class)\n",
    "# =========================================\n",
    "\n",
    "# Sample log data (simulate Apache log entries)[Similar example we did in class]\n",
    "log_data = [\n",
    "    \"10.0.0.1 - - [10/Oct/2023:13:55:36 +0000] \\\"GET /index.html HTTP/1.1\\\" 200 1043\",\n",
    "    \"10.0.0.2 - - [10/Oct/2023:13:55:40 +0000] \\\"GET /login HTTP/1.1\\\" 200 2048\",\n",
    "    \"10.0.0.1 - - [10/Oct/2023:13:56:10 +0000] \\\"GET /dashboard HTTP/1.1\\\" 500 512\",\n",
    "    \"10.0.0.3 - - [10/Oct/2023:13:56:42 +0000] \\\"GET /profile HTTP/1.1\\\" 200 1337\",\n",
    "    \"10.0.0.2 - - [10/Oct/2023:13:57:01 +0000] \\\"GET /index.html HTTP/1.1\\\" 404 321\"\n",
    "]\n",
    "\n",
    "logs_rdd = spark.sparkContext.parallelize(log_data)\n",
    "\n",
    "# 1. Count requests per IP address\n",
    "ip_counts = logs_rdd.map(lambda line: (line.split(\" \")[0], 1))\n",
    "ip_summary = ip_counts.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Request Count per IP:\", ip_summary.collect())\n",
    "\n",
    "# 2. Count status codes (200, 404, 500)\n",
    "status_counts = logs_rdd.map(lambda line: (line.split(\" \")[-2], 1))\n",
    "status_summary = status_counts.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Status Code Summary:\", status_summary.collect())\n",
    "\n",
    "# 3. Count access frequency for each endpoint\n",
    "endpoint_counts = logs_rdd.map(lambda line: line.split(\" \")[6]) \\\n",
    "                          .map(lambda endpoint: (endpoint, 1)) \\\n",
    "                          .reduceByKey(lambda a, b: a + b)\n",
    "print(\"Endpoint Access Counts:\", endpoint_counts.collect())\n",
    "\n",
    "# 4. Filter and count only failed (status != 200) requests\n",
    "failed_logs = logs_rdd.filter(lambda line: line.split(\" \")[-2] != \"200\")\n",
    "failed_endpoints = failed_logs.map(lambda line: (line.split(\" \")[6], 1)) \\\n",
    "                               .reduceByKey(lambda a, b: a + b)\n",
    "print(\"Failed Endpoint Hits:\", failed_endpoints.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session [Good practice when you are done to stop the background running of spark]\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
